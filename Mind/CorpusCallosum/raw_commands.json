{
    "command_types": {
        "ASR": {
            "name": "Automatic Speech Recognition",
            "description": "Process audio input to convert speech to text",
            "parameters": {
                "input_audio": {
                    "type": "bytes",
                    "required": true,
                    "description": "Raw audio data to process"
                },
                "language": {
                    "type": "string",
                    "required": false,
                    "default": "en",
                    "description": "Language code for speech recognition"
                },
                "model_type": {
                    "type": "string",
                    "required": false,
                    "default": "base",
                    "description": "Type of ASR model to use"
                }
            }
        },
        "TTS": {
            "name": "Text to Speech",
            "description": "Convert text to speech audio",
            "parameters": {
                "text": {
                    "type": "string",
                    "required": true,
                    "description": "Text to convert to speech"
                },
                "voice_id": {
                    "type": "string",
                    "required": false,
                    "default": "default",
                    "description": "Voice ID to use for synthesis"
                },
                "speed": {
                    "type": "float",
                    "required": false,
                    "default": 1.0,
                    "description": "Speech speed multiplier"
                },
                "pitch": {
                    "type": "float",
                    "required": false,
                    "default": 1.0,
                    "description": "Speech pitch multiplier"
                }
            }
        },
        "VAD": {
            "name": "Voice Activity Detection",
            "description": "Detect speech activity in audio",
            "parameters": {
                "audio_chunk": {
                    "type": "bytes",
                    "required": true,
                    "description": "Audio chunk to analyze"
                },
                "threshold": {
                    "type": "float",
                    "required": false,
                    "default": 0.5,
                    "description": "Detection threshold"
                },
                "frame_duration": {
                    "type": "integer",
                    "required": false,
                    "default": 30,
                    "description": "Frame duration in milliseconds"
                }
            }
        },
        "LLM": {
            "name": "Large Language Model",
            "description": "Process text through language model",
            "parameters": {
                "request_id": {
                    "type": "string",
                    "required": true,
                    "description": "Unique request identifier"
                },
                "work_id": {
                    "type": "string",
                    "required": true,
                    "default": "llm",
                    "description": "Work identifier"
                },
                "action": {
                    "type": "string",
                    "required": true,
                    "default": "inference",
                    "description": "Action to perform"
                },
                "object": {
                    "type": "string",
                    "required": true,
                    "default": "llm.utf-8.stream",
                    "description": "Object to process"
                },
                "data": {
                    "type": "object",
                    "required": true,
                    "description": "Command data",
                    "parameters": {
                        "delta": {
                            "type": "string",
                            "required": true,
                            "description": "Input text prompt"
                        },
                        "index": {
                            "type": "integer",
                            "required": true,
                            "default": 0,
                            "description": "Response index"
                        },
                        "finish": {
                            "type": "boolean",
                            "required": true,
                            "default": true,
                            "description": "Whether this is the final response"
                        }
                    }
                }
            },
            "command_formats": {
                "setup": {
                    "request_id": "llm_setup",
                    "work_id": "llm",
                    "action": "setup",
                    "object": "llm.setup",
                    "data": {
                        "model": "qwen2.5-0.5b",
                        "response_format": "llm.utf-8",
                        "input": "llm.utf-8",
                        "enoutput": true,
                        "enkws": false,
                        "max_token_len": 127,
                        "prompt": "You are a helpful assistant."
                    }
                },
                "inference": {
                    "request_id": "llm_inference",
                    "work_id": "llm",
                    "action": "inference",
                    "object": "llm.utf-8",
                    "data": {
                        "delta": "What is the capital of France?",
                        "index": 0,
                        "finish": true
                    }
                },
                "exit": {
                    "request_id": "llm_exit",
                    "work_id": "llm",
                    "action": "exit",
                    "object": "None",
                    "data": null
                }
            }
        },
        "VLM": {
            "name": "Vision Language Model",
            "description": "Process images through vision-language model",
            "parameters": {
                "image_data": {
                    "type": "bytes",
                    "required": true,
                    "description": "Image data to process"
                },
                "prompt": {
                    "type": "string",
                    "required": true,
                    "description": "Text prompt for image analysis"
                }
            }
        },
        "KWS": {
            "name": "Keyword Spotting",
            "description": "Detect keywords in audio",
            "parameters": {
                "audio_data": {
                    "type": "bytes",
                    "required": true,
                    "description": "Audio data to analyze"
                },
                "keywords": {
                    "type": "array",
                    "required": true,
                    "items": {
                        "type": "string"
                    },
                    "description": "List of keywords to detect"
                }
            }
        },
        "SYS": {
            "name": "System Commands",
            "description": "System-level operations",
            "parameters": {
                "action": {
                    "type": "string",
                    "required": true,
                    "description": "System action to perform",
                    "enum": ["ping", "setup", "init", "config", "initialize", "status", "reset"]
                },
                "data": {
                    "type": "object",
                    "required": false,
                    "description": "Additional data for the action"
                }
            },
            "command_formats": {
                "setup": {
                    "request_id": "sys_setup", 
                    "work_id": "sys",
                    "action": "setup",
                    "object": "None",
                    "data": {
                        "system_message": "You are a helpful assistant.",
                        "enkws": true,
                        "model": "default",
                        "enoutput": true,
                        "version": "1.0",
                        "max_token_len": 2048,
                        "wake_word": "hey penphin"
                    }
                },
                "ping": {
                    "request_id": "sys_ping",
                    "work_id": "sys",
                    "action": "ping",
                    "object": "None",
                    "data": "None"
                }
            }
        },
        "AUDIO": {
            "name": "Audio Processing",
            "description": "General audio processing operations",
            "parameters": {
                "action": {
                    "type": "string",
                    "required": true,
                    "description": "Audio processing action"
                },
                "audio_data": {
                    "type": "bytes",
                    "required": true,
                    "description": "Audio data to process"
                }
            }
        },
        "CAMERA": {
            "name": "Camera Control",
            "description": "Camera operations and control",
            "parameters": {
                "action": {
                    "type": "string",
                    "required": true,
                    "description": "Camera action to perform"
                },
                "resolution": {
                    "type": "object",
                    "required": false,
                    "description": "Camera resolution settings"
                }
            }
        },
        "YOLO": {
            "name": "Object Detection",
            "description": "YOLO-based object detection",
            "parameters": {
                "image_data": {
                    "type": "bytes",
                    "required": true,
                    "description": "Image data to analyze"
                },
                "confidence_threshold": {
                    "type": "float",
                    "required": false,
                    "default": 0.5,
                    "description": "Detection confidence threshold"
                }
            }
        },
        "whisper": {
            "name": "Whisper Speech Recognition",
            "description": "Whisper-based speech recognition",
            "parameters": {
                "audio_data": {
                    "type": "bytes",
                    "required": true,
                    "description": "Audio data to transcribe"
                },
                "language": {
                    "type": "string",
                    "required": false,
                    "default": "en",
                    "description": "Language code for transcription"
                },
                "model_type": {
                    "type": "string",
                    "required": false,
                    "default": "base",
                    "description": "Whisper model type"
                }
            }
        },
        "melotts": {
            "name": "Neural TTS",
            "description": "Melotron-based text-to-speech",
            "parameters": {
                "text": {
                    "type": "string",
                    "required": true,
                    "description": "Text to synthesize"
                },
                "voice_id": {
                    "type": "string",
                    "required": false,
                    "default": "default",
                    "description": "Voice ID to use"
                }
            }
        }
    }
} 